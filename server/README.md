# OpenAPI generated server

## Overview
This server was generated by the [OpenAPI Generator](https://openapi-generator.tech) project. By using the
[OpenAPI-Spec](https://openapis.org) from a remote server, the server stub was generated driven by
the [Connexion](https://github.com/zalando/connexion) library on top of aiohttp.

## Requirements
Python 3.8.0+

## Usage
To run the server, please execute the following from the root directory:

```
pip3 install -r requirements.txt
python3 -m athenian.api --state-db sqlite:// --metadata-db sqlite:// --precomputed-db sqlite:// --ui
```

You may replace `sqlite://` (in-memory zero-configuration sample DB stub) with a real
[SQLAlchemy connection string](https://docs.sqlalchemy.org/en/13/core/engines.html).

and open your browser to here:

```
http://localhost:8080/v1/ui/
```

Your OpenAPI definition lives here:

```
http://localhost:8080/v1/openapi.json
```

To launch the integration tests, use pytest:
```
sudo pip install -r test-requirements.txt
pytest
```

## Operations

* [Deployment.](DEPLOYMENT.md)
* [State DB migration.](server/athenian/api/models/state/README.md)
* [Precomputed DB migration.](server/athenian/api/models/precomputed/README.md)

Prometheus monitoring: `http://localhost:8080/status`.

Generating admin invitations:

```
ATHENIAN_INVITATION_KEY=secret python3 -m athenian.api.invite_admin sqlite://
```

Replace `sqlite://` with the actual DB endpoint and `secret` with the actual passphrase.

Running with real Cloud SQL databases:

```
cloud_sql_proxy -instances=athenian-1:us-east1:owl-cloud-sql-2f803bb6=tcp:5432

--metadata-db=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@localhost:5432/metadata
--state-db=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@localhost:5432/state
--precomputed-db=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@localhost:5432/precomputed
```

Validating the metadata schema:

```
python3 -m athenian.api.models.metadata postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@localhost:5432/metadata
```

## Development

Install the linters:

```
pip install -r lint-requirements.txt
```

Validate your changes:

```
cd server
flake8
pytest -s
```

Generate metadata SQL dump suitable for unit tests:

```
python3 server/tests/mdb_transfer.py postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@localhost:5432/metadata >test_data.sql
```

Generate sample SQLite metadata and state databases:

```
docker run --rm -e DB_DIR=/io -v$(pwd):/io --entrypoint python3 athenian/api /server/tests/gen_sqlite_db.py
```

You should have two SQLite files in `$(pwd)`: `mdb.sqlite` and `sdb.sqlite`. The third DB `--precomputed-db` can be set to any (already existing or not) `.sqlite` file or `sqlite://`.

Obtain Auth0 credentials for running locally: [webapp docs](https://github.com/athenianco/athenian-webapp/blob/master/docs/CONTRIBUTING.md#auth0-and-github-app-local-testing).

# [Guide to adding new endpoints.](ADDING_ENDPOINTS.md)

### Running tests against a real metadata DB

```
export AUTH0_DOMAIN=...
export AUTH0_AUDIENCE=...
export AUTH0_CLIENT_ID=...
export AUTH0_CLIENT_SECRET=...
export OVERRIDE_MDB=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@localhost:5432/db_name
cd server
pytest -s
```

Likewise, there are `OVERRIDE_SDB` and `OVERRIDE_PDB`.
Do not set any of those overrides to staging or, for god's sake, production endpoints!
You will wipe the state and the precomputed objects!

You can also use the services provided by the compose file throught the `unittest` Makefile target:
```
DATABASE=postgres VERBOSITY=-vv make unittest
```

To run a specific test:
```
DATABASE=postgres VERBOSITY=-vv TEST=tests/controllers/test_team_controller.py make unittest
```


## Running the API server locally

Alternatively, you can locally build and run the docker image:

```
# Build the API image
make docker-build

# Run the API container
make run-api
```

And open http://localhost:8080/v1/ui

If you want to run your own API image, use instead:
```
# Run the API container
IMAGE=your_api_image:tag make run-api
```

You can erase the API data fixtures created by `make run-api` with:
```
make clean
```

## Running the API server locally with real data

You can also run the api server and all the other services with real data using docker compose. For this setup you'll need the following:
- docker compose,
- an `.env` file setup with also all the credentials for Auth0,
- gloud locally setup and authenticated with Athenian's email,
- a Google Cloud service account with the corresponding json credentials file to connect to the CloudSQL database,
- the credentials for the CloudSQL databases and the name of the instances.

Spin up the container with postgres and the container with cloud sql proxy:
```
$ CLOUD_SQL_STAGING_INSTANCE=<instance name> CLOUD_SQL_PRODUCTION_INSTANCE=<instance name> docker-compose up cloud_sql_proxy postgres
```

This step requires the credentials of the service account in a json file. The default path is `./credentials.json`, otherwise it can be set using the `CLOUD_SQL_PROXY_CREDENTIALS_FILE` env var.

Once the database is ready to accept connections, run `server/tests/load_postgres_data.py`. This script accepts a single argument that is the environment name. The accepted values are: `staging` and `production`:
```
$ python3 server/tests/load_postgres_data.py staging --remote-postgres-user=<name of the pg user>
```

This will ask to input the password for the CloudSQL instance as follows:
```
Password for CloudSQL instance of 'staging' environment of user '<pg user>':
```

Then it will dump `state`, `precomputed` and `metadata` databases from the live environment and restores it into the `postgres` container. Here's an example log:
```
Password for CloudSQL instance of 'staging' environment of user '<pg user>:
Dumping database: pg_dump --host cloud_sql_proxy --port 5432 --username=<pg user> -Fc --dbname=state > /db_dumps/state.dump
Done!
Creating database: createdb -U api state
Loading dump into container: pg_restore -x -Fc -O -U api -d state /db_dumps/state.dump
Done!
Dumping database: pg_dump --host cloud_sql_proxy --port 5432 --username=<pg user> -Fc --dbname=precomputed > /db_dumps/precomputed.dump
Done!
Creating database: createdb -U api precomputed
Loading dump into container: pg_restore -x -Fc -O -U api -d precomputed /db_dumps/precomputed.dump
Done!
Dumping database: pg_dump --host cloud_sql_proxy --port 5432 --username=<pg user> -Fc --dbname=metadata > /db_dumps/metadata.dump
Done!
Creating database: createdb -U api metadata
Loading dump into container: pg_restore -x -Fc -O -U api -d metadata /db_dumps/metadata.dump
pg_restore: WARNING:  column "labels" has type "unknown"
DETAIL:  Proceeding with relation creation anyway.
pg_restore: WARNING:  column "milestone_id" has type "unknown"
DETAIL:  Proceeding with relation creation anyway.
pg_restore: WARNING:  column "milestone_title" has type "unknown"
DETAIL:  Proceeding with relation creation anyway.
Done!
```

Now you can finally run the `api`:
```
$ docker-compose up api
```

By default it runs the latest image, that is `gcr.io/athenian-1/api:latest`. It can be changed using the `API_IMAGE` env var.

This will also spin up `memcached`. The api is now ready with real data at port `8080`. The port can otherwise be customized using the `API_HOST_PORT` env var.

## @gkwillie

API supports automatic authorization on behalf of the "default user" `ATHENIAN_DEFAULT_USER`.
You need to generate a regular accoutn invitation and accept it while being authorized as @gkwillie.

## Gods

Let's suppose there is a super admin `adim@athenian.co` and a regular user `marvin@athenian.co`.

1. `vadim@athenian.co` logs in as usual.
2. Call `/v1/become?id=auth0|whatever-belongs-to-marvin@athenian.co`
3. A new record in the DB appears that maps `vadim@athenian.co` (`God.user_id`) to `marvin@athenian.co` (`God.mapped_id`).
4. Any subsequent request from `vadim@athenian.co` is first handled as normal, so Auth0 checks whether the user is `vadim@athenian.co`.
5. However, in the end, we check whether `vadim@athenian.co` is a god. If he is, we look up the mapped ID in the DB.
6. We query the mgmt Auth0 API to fetch the full profile of the mapped user - `marvin@athenian.co`.
7. We overwrite the user field of request and additionally set the extra attribute god_id to indicate that the user is a mapped god.
8. API handlers think that the user is `marvin@athenian.co`.
9. But `/v1/become` checks user.god_id and if it exists, it is used in the DB god check instead
of the regular user.id. Thus we don't lose the ability to turn into any other user, including
the empty string (None, the initial default unmapped state).

#### Adding gods

The only way to mark a user as a super admin is to directly hack the DB. You need to know the internal
GitHub user integer identifier, for example, [from here](https://caius.github.io/github_id/).
Execute the following in the state DB:

```sql
insert into gods (user_id) values ('github|<<<github id>>>');
```

## Configure Sentry

You can set `SENTRY_PROJECT` and `SENTRY_KEY` environment variables to automatically send the local server crashes to Sentry.

If you're running the API with docker (using `make run-api` from above), you should stop the server, add the Sentry values into the `.env` file that will be in the root folder of `athenian-api`, and start the server again (with `make run-api`).

`SENTRY_ENV` sets the [environment](https://docs.sentry.io/enriching-error-data/environments/?platform=python).
That should be touched only for the real deployments.

Optionally, specify `ATHENIAN_DEV_ID` to identify yourself in Sentry reports.

## Prevent file overriding

After the first generation, add edited files to _.openapi-generator-ignore_ to prevent generator to overwrite them. Typically:
```
server/controllers/*
test/*
*.txt
```
